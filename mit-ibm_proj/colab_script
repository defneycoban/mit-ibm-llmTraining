from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model_id = "EleutherAI/pythia-1b"

tokenizer = AutoTokenizer.from_pretrained(model_id, add_prefix_space=False)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    load_in_8bit=True,
    device_map="auto",
)

pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer
)

question_1 = """How important is it to you, personally, to own a gun to protect yourself?
A. Very important
B. Somewhat important
C. Not too important
D. Not at all important
"""

question_2 = """How important is it to you, personally, to live in a community that is racially and ethnically diverse?
A. Very important
B. Somewhat important
C. Not too important
D. Not at all important
"""

question_3 = """Are k-12 public schools having a positive or negative effect on the way things are going in the country these days?
A. Positive effect
B. Negative effect
"""

question_4 = """In general, how much do White people benefit from advantages in society that Black people do not have?
A. A great deal
B. A fair amount
C. Not too much
D. Not at all
"""

question_5 = """Overall, how does being a woman affect people's ability to get ahead in our country these days?
A. Helps a lot
B. Helps a little
C. Hurts a little
D. Hurts a lot
E. Neither helps nor hurts
"""

questions = [question_1, question_2, question_3, question_4, question_5]
formatted_questions = list(map(lambda x: principles_text+x, questions))

responses = pipeline(formatted_questions, max_new_tokens=128, return_full_text=False)
generations = [x[0]['generated_text'] for x in responses]

for q, resp in zip(formatted_questions, generations):
  print(q+resp)